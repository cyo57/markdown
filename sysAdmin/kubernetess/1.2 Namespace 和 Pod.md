---
tags:
  - k8s
Created: "250429"
Modified: "250430"
---

## 环境配置
### metrics-server

> Metrics-server 收集节点和 Pod 的资源使用指标，并通过 Metrics API 提供这些数据，是 Kubernetes 自动化伸缩和资源监控的基础。它使得 HPA、VPA 以及 `kubectl top` 命令能够获取实时的 CPU 和内存使用情况。

拉取官方 [yaml](https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.7.2/components.yaml)

修改配置文件

```yaml
spec:
      containers:
      - args:
        # 忽略TLS
        - --kubelet-insecure-tls
		# 修改镜像源
        image: registry.cn-hangzhou.aliyuncs.com/cloudcs/metrics-server:v0.7.2
```

#### 基本命令

查看所有节点的资源使用

```shell
kubectl top nodes

root@k8smaster:~# kubectl top node
NAME        CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   
k8smaster   101m         5%     2205Mi          58%       
k8snode1    38m          1%     1291Mi          34%       
k8snode2    38m          1%     1248Mi          33%       
root@k8smaster:~# kubectl top po
NAME                       CPU(cores)   MEMORY(bytes)   
deploy1-688964868b-655bg   0m           8Mi             
deploy1-688964868b-bwk92   0m           1Mi             
deploy1-688964868b-dt7sv   0m           8Mi             
```

## Namespace

> Namespace 在 Kubernetes 中提供了一种机制，用于将集群中的资源（如 Pod、Service、Deployment 等）划分为相互隔离的逻辑组。
> 
>  不同 namespace 的 pod 相互隔离，避免不同团队或应用之间的命名冲突，并可以用于资源配额管理。

### 列出 namespace

```shell
kubectl get namespaces

root@k8smaster:~# kubectl get namespaces
NAME               STATUS   AGE
calico-apiserver   Active   92m
calico-system      Active   92m
default            Active   16h
kube-node-lease    Active   16h
kube-public        Active   16h
kube-system        Active   16h
tigera-operator    Active   92m
```

### 创建 namespace

```shell
kubectl create namespace <namespace-name>

root@k8smaster:~# kubectl create namespace cloudhnjm
namespace/cloudhnjm created
root@k8smaster:~# kubectl get ns
NAME               STATUS   AGE
calico-apiserver   Active   93m
calico-system      Active   93m
cloudhnjm          Active   8s
default            Active   16h
kube-node-lease    Active   16h
kube-public        Active   16h
kube-system        Active   16h
tigera-operator    Active   93m
```

也可以通过 YAML 文件创建，`kubectl apply -f my-app-ns.yaml`

my-app-ns.yaml
```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: my-app
```

### 删除 namespace

```shell
kubectl delete namespace

root@k8smaster:~# kubectl create namespace temp
namespace/temp created
root@k8smaster:~# kubectl delete namespace temp
namespace "temp" deleted
root@k8smaster:~#
```

### 查看当前所在的命名空间

```shell
kubectl config get-contexts

root@k8smaster:~# kubectl config get-contexts
CURRENT   NAME                          CLUSTER      AUTHINFO           NAMESPACE
*         kubernetes-admin@kubernetes   kubernetes   kubernetes-admin   
```

### 切换命名空间

```shell
kubectl config set-context --current --namespace kube-system

root@k8smaster:~# kubectl config set-context --current --namespace kube-system
Context "kubernetes-admin@kubernetes" modified.
root@k8smaster:~# kubectl config get-contexts
CURRENT   NAME                          CLUSTER      AUTHINFO           NAMESPACE
*         kubernetes-admin@kubernetes   kubernetes   kubernetes-admin   kube-system
root@k8smaster:~# 
root@k8smaster:~# kubectl get po -A
NAMESPACE          NAME                                      READY   STATUS    RESTARTS       AGE
calico-apiserver   calico-apiserver-77b9c64bc5-59ctd         1/1     Running   4 (66m ago)    69m
calico-apiserver   calico-apiserver-77b9c64bc5-d57xm         1/1     Running   3 (67m ago)    69m
calico-system      calico-kube-controllers-b69db776c-xhtb8   1/1     Running   0              69m
calico-system      calico-node-cth2r                         1/1     Running   0              69m
calico-system      calico-node-mm9ct                         1/1     Running   0              69m
calico-system      calico-node-txssf                         1/1     Running   0              69m
calico-system      calico-typha-6cdcbdcd9-72ghb              1/1     Running   0              69m
calico-system      calico-typha-6cdcbdcd9-w5gxk              1/1     Running   0              68m
calico-system      csi-node-driver-bpqm2                     2/2     Running   0              69m
calico-system      csi-node-driver-ndm7v                     2/2     Running   0              69m
calico-system      csi-node-driver-rhlxr                     2/2     Running   0              69m
kube-system        coredns-66f779496c-64q8g                  1/1     Running   1 (122m ago)   16h
kube-system        coredns-66f779496c-b6vs6                  1/1     Running   1 (122m ago)   16h
kube-system        etcd-k8smaster                            1/1     Running   3 (122m ago)   16h
kube-system        kube-apiserver-k8smaster                  1/1     Running   3 (122m ago)   16h
kube-system        kube-controller-manager-k8smaster         1/1     Running   3 (122m ago)   16h
kube-system        kube-proxy-chwzb                          1/1     Running   3 (80m ago)    16h
kube-system        kube-proxy-tpjm5                          1/1     Running   1 (122m ago)   16h
kube-system        kube-proxy-vcnz7                          1/1     Running   2 (81m ago)    16h
kube-system        kube-scheduler-k8smaster                  1/1     Running   3 (122m ago)   16h
kube-system        metrics-server-5796b7676f-d7lcs           1/1     Running   0              12m
tigera-operator    tigera-operator-c78c656bf-8krn9           1/1     Running   0              69m
```

| 名称        | 含义                                                                                                                           |
| --------- | ---------------------------------------------------------------------------------------------------------------------------- |
| NAMESPACE | 所属的命名空间                                                                                                                      |
| NAME      | Pod 的名称，在同一个命名空间内是唯一的                                                                                                        |
| READY     | `1/1` 表示 Pod 中有 1 个容器且已就绪                                                                                                    |
| STATUS    | Pod 的当前状态<br>常见的有 `Running` (正在运行), `Pending` (等待调度), `Succeeded` (成功完成), `Failed` (失败), `CrashLoopBackOff` (容器崩溃后正在重启循环) 等。 |
| RESTARTS  | Pod 中容器重启的总次数。括号中的时间表示最后一次重启发生的时间距离现在多久。                                                                                     |
| AGE       | Pod 的运行时间（当 Pod 中的容器重启时，`AGE` 不会刷新。容器重启是 Pod 内部发生的事情，Pod 对象本身并没有被删除重建，所以它的创建时间不会改变，`AGE` 会继续增长。）                             |
| RESTARTS  | Pod 中容器重启的总次数                                                                                                                |
| AGE       | Pod 运行了多久                                                                                                                    |


因为 k8s 切换命名空间较为繁琐，可以使用社区工具 [kubectx](https://github.com/ahmetb/kubectx) 中的 `kubens` 进行切换

```shell
chmod +x kubens
mv kubenv /usr/local/bin/kubens
```

### 指定命令在特定的 Namespace 中执行

当执行 `kubectl get`, `kubectl apply`, `kubectl delete`, `kubectl logs` 等命令时，如果不指定 Namespace，它默认会在当前的上下文 (Context) 设置的 Namespace 中执行

对特定 Namespace 中的资源进行操作，使用 `-n` 或 `--namespace` 标志。

```shell
kubectl get pods -n <namespace-name>        # 查看指定 Namespace 下的 Pods
kubectl get services -n kube-system         # 查看 kube-system Namespace 下的 Services
kubectl apply -f my-resource.yaml -n my-app # 在 my-app Namespace 中创建资源
kubectl delete pod <pod-name> -n my-app     # 删除 my-app Namespace 下的 Pod
kubectl logs <pod-name> -n my-app           # 查看 my-app Namespace 下 Pod 的日志
```

如果你想查看**所有** Namespace 下的某种资源，可以使用 `-A` 或 `-all-namespaces`

```shell
kubectl get services -A
kubectl get pod -A
```


## Pod

> Pod 是 Kubernetes 中最小的可部署单元，它是应用程序的单个实例或一组紧密关联的容器的集合。
> 同一个 Pod 中的容器共享网络命名空间、存储卷等资源，它们通常一起调度到同一个节点上，并且应该被视为一个单一的管理单元。

> 通常一个 Pod 推荐只有一个容器

### 创建 Pod

创建 Pod 可以通过 CLI 和 Yaml 文件，推荐后者

#### 通过 CLI

```shell
kubectl run <PodName> --image <ImageName>

root@k8smaster:~# kubectl run nginx-1 --image nginx
pod/nginx-1 created
root@k8smaster:~# kubectl get po
NAME      READY   STATUS              RESTARTS   AGE
nginx-1   0/1     ContainerCreating   0          21s
root@k8smaster:~# kubectl get po
NAME      READY   STATUS    RESTARTS   AGE
nginx-1   1/1     Running   0          25s
root@k8smaster:~# 
```

#### 查看 Pod 信息

查看所在节点和IP

```shell
kubectl get pod -o wide

root@k8smaster:~# kubectl get pod -o wide
NAME      READY   STATUS    RESTARTS   AGE     IP               NODE       NOMINATED NODE   READINESS GATES
nginx-1   1/1     Running   0          2m10s   10.244.185.197   k8snode2   <none>           <none>
```

查看描述信息

```shell
kubectl describe pod <PodName>
```

#### 镜像下载策略 --image-pull-policy

 > 决定了 Kubelet 何时尝试从镜像仓库拉取镜像。

- Always
	- 每次启动都尝试拉取最新，即使本地已经存在也会重新拉取摘要（如果版本一样则不重新拉取）
- IfNotPresent
	- 默认策略
	- 如果本地存在镜像，就不会拉取远端最新镜像。
- Never
	- 永远不拉取镜像，如果本地不存在，则会启动失败

```shell
kubectl run <PodName> --image <ImageName> --image-pull-policy <ImagePullPolicy>
```

#### 通过 yaml

yaml文件不需要从0开始编写，可以 通过命令生成一个基础版本，在此版本上进行修改，增加配置

```shell
kubectl run pod-test --image nginx --image-pull-policy IfNotPresent --dry-run=client -o yaml > pod-test.yaml
```

pod-test.yaml
```yaml
apiVersion: v1
kind: Pod # 资源类型
metadata:
  creationTimestamp: null # 创建时间戳，null会系统自动生成
  labels: # 标签
    run: pod-test
  name: pod-test # Pod名称，在同Namespace中唯一
spec:
  # 容器列表（一个pod可包含多容器）
  containers:
  - image: nginx
    imagePullPolicy: IfNotPresent
    name: pod-test # Pod内的容器名称，在同Pod内唯一
    resources: {} # 资源限制（CPU、内存等）。这里未指定
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
```

使用 `kubectl apply -f <File>` 创建/更新现有资源
使用 `kubectl create -f <File>` 创建资源，无法更新资源


### 进入 Pod

#### 通过 Pod 名称

```shell
kubectl exec -it <PodName> -- <Command>
```

> 添加 `--` 可以让 `kubectl` 明确知道 `<Command>` 的参数传递给容器，而不是作为 `kubectl exec` 的参数
> `--` 之后的内容会被当作要执行的命令及其参数传递给容器内的进程。

#### 通过 Yaml

```shell
kubectl exec -it -f <File> -- <Command>
```

### 删除 Pod

#### 通过 CLI

```
kubectl delete pod <PodName>
```

#### 通过 yaml

```shell
kubectl delete -f <File>
```

### Pod 生命周期

> 一个 Pod 的生命周期，就像一个 Docker 容器从创建到运行再到结束的过程，只不过在 Kubernetes 里，这个过程被管理得更精细。

- **Pending**：等待中，Kubernetes 需要决定把这个 Pod 放在哪个节点（也就是哪台机器）上运行，这个过程叫“调度”。然后对应节点的 Kubelet 需要去拉取容器镜像，Pod 就会处于这个 Pending 状态。
- **Running**：容器成功启动
- **Succeeded**：如果退出码是 0（表示成功），那么 Pod 的状态就会变成 Succeeded。
- **Failed**：Pod 里的容器在运行过程中崩溃了，或者以非零的退出码结束
- **CrashLoopBackOff** ：如果一个 Pod 里的容器启动后很快就崩溃并退出了（进入 Failed 状态），Kubernetes 会尝试重启它。如果它反复地启动、崩溃、再启动、再崩溃，就会进入 CrashLoopBackOff 状态。Kubernetes 会以指数退避的方式（等待时间越来越长）尝试重启，避免无限循环地快速重启耗尽资源

#### restartPolicy

> 当 Pod 里的容器停止运行时的行为

在一个默认 yaml 中，Always作为默认值

```yaml
apiVersion: v1
kind: Pod # 资源类型
metadata:
  creationTimestamp: null # 创建时间戳，null会系统自动生成
  labels: # 标签
    run: pod-test
  name: pod-test # Pod名称，在同Namespace中唯一
spec:
  # 容器列表（一个pod可包含多容器）
  containers:
  - image: nginx
    imagePullPolicy: IfNotPresent
    name: pod-test # Pod内的容器名称，在同Pod内唯一
    resources: {} # 资源限制（CPU、内存等）。这里未指定
  dnsPolicy: ClusterFirst # DNS策略
  restartPolicy: Always
status: {}
```

- **Always**：不管容器是因为什么原因停止的（正常退出，或者崩溃），Kubernetes 都会尝试重新启动它。
- **OnFailure**：只有当容器以非零的退出码（表示失败）停止时，Kubernetes 才会尝试重新启动它。
- **Never**：容器停止后，Kubernetes 永远不会尝试重新启动它

#### apiversion 和 api-resources

可以通过命令进行查看KIND，APIVERSION，NAME之间的关系

```shell
kubectl api-versions

kubectl api-resources
```


#### 静态 Pod

> 想象一下，在一个服务器上直接用 `docker run` 命令启动了一个容器。这个容器的生命周期完全由你在这个服务器上的操作（比如 `docker stop`）来控制。而 Kubernetes 的 API Server（就是你平时用 `kubectl` 交互的那个中心大脑）并不知道这个容器是你直接启动的，它无法管理它。
> 
> **静态 Pod** 就有点像这种情况，但它是被 **Kubelet**（运行在每个 Kubernetes 节点上的代理程序）直接管理的，而不是通过 Kubernetes API Server 创建和管理的。

> [!tip]
> 静态 Pod需要在 Node 节点测试，
> 由 kubelet 根据定义的文件通常是在 /etc/kubernetes/manifests）来创建和管理

```shell
root@k8smaster:~# ls /etc/kubernetes/manifests/
etcd.yaml  kube-apiserver.yaml  kube-controller-manager.yaml  kube-scheduler.yaml
```

- **etcd**：分布式键值存储系统，存储了集群的所有状态数据（比如Pod, Service, Deployment 等等）
- **kube-apiserverl**：所有对集群的操作都必须通过 API Server（比如`kubectl get po`）
- **kube-controller-manager**：监控集群的当前状态，比如定义了 Deploment 运行 3 个 Nginx Pod，由该服务确保始终有 3 个 Pod 在运行
- **kube-scheduler**：负责调度新的 Pod，当新的 Pod 创建时 scheduler 会根据各种因素来决定 Pod 运行在哪个 Node 上

### 标签 Label

> 每个标签都是一个**“键值对”（key: value）
> k8s 的 Label 是 k8s **内部**用来组织、管理和连接各种资源（比如 Service 找 Pod）的**核心机制**。

**为什么需要 Label？**

1. **识别和分组：** 你可以用 Label 来标记和分组你的对象。比如，所有属于同一个应用的 Pod 都打上 `app: my-web-app` 的标签；所有运行在生产环境的 Pod 都打上 `env: production` 的标签。
2. **选择：** k8s 里很多其他对象需要“找到”特定的对象来操作。它们就是通过 Label 来“选择”的。
#### 查看资源的标签

添加参数 `--show-labels` 即可

```shell
root@k8smaster:~# kubectl get node k8snode1 --show-labels
NAME       STATUS   ROLES    AGE   VERSION    LABELS
k8snode1   Ready    <none>   24h   v1.28.15   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8snode1,kubernetes.io/os=linux

root@k8smaster:~# kubectl get pod pod-test --show-labels 
NAME       READY   STATUS    RESTARTS      AGE     LABELS
pod-test   1/1     Running   2 (27m ago)   6h16m   run=pod-test
```

#### 设置标签

##### 通过 CLI

```shell
kubectl label <Resource> <Name> <LabelName> # 添加标签
kubectl label <Resource> <Name> <LabelName>- # 移除标签
```

##### 通过 yaml

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-pod-with-labels
  labels: # <-- 这里是标签部分
    app: my-web-app
    env: development
    version: v1.0.0
    key: value

```

#### 筛选标签

使用命令 `kubectl get <Resource> -l <Labels>` 进行筛选

```shell
kubectl get pods -l app # 存在app键
kubectl get pods -l !app # 不存在app键
kubectl get pods -l app=frontend # 存在键值对
kubectl get pods -l app=frontend,env=production # 查询多个
kubectl get pods -l 'app in (production, development)' # 或运算
kubectl get pods -l 'app notin (production, development)'
```

#### 指定 Pod 运行在某个节点上


```yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: pod2
  name: pod2
spec:
  nodeSelector: # node 选择
    disktype: ssdnvme # 通过这里查询 key: disktype, value: ssdnvme
  containers:
  - image: nginx
    imagePullPolicy: IfNotPresent
    name: pod2
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
```

### 注解 Annotations

> 注解是 Kubernetes 中用来给对象（比如 Pod、Service 等）附加**非识别性元数据**的一种方式。
> 
> 它们是键值对，就像标签（Labels）一样。用来存储一些**描述性信息**，比如工具、库或配置信息，这些信息对用户或工具很有用，但 Kubernetes **不会**用它们来识别或选择对象。

可以在一个 Pod 上添加注解，记录这个 Pod 是由哪个 Git Commit 构建的，或者是由哪个自动化工具部署的。

#### 通过 yaml

关键字 `annotations`

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-annotated-pod
  labels:
    app: demo # 这是一个标签
  annotations: # <-- 这里是注解部分
    description: "这是一个带有描述性注解的示例 Pod"
    managed-by: "kubectl-apply"
    build-info/git-commit: "abcdef123456" # 注解的键可以使用斜杠
spec:
  containers:
  - name: nginx
    image: nginx:latest
    ports:
    - containerPort: 80
```

#### 通过 CLI

添加 / 修改

```shell
kubectl annotate pod my-annotated-pod owner="dev-team-a"
```

移除

```shell
kubectl annotate pod my-annotated-pod managed-by- # 在注解键后面加一个短横线
```

### cordon 警戒

> [!IMPORTANT]
> 用来标记一个节点（Node）为**不可调度（Unschedulable）**
> 新的 Pod 不会再调度到该节点，已经在该节点运行的 Pod 不受影响

使用命令 `kubectl cordon <nodeName>` 设置

```shell
root@k8smaster:~# kubectl cordon k8snode1
node/k8snode1 cordoned
root@k8smaster:~# kubectl get no
NAME        STATUS                     ROLES           AGE    VERSION
k8smaster   Ready                      control-plane   135m   v1.28.15
k8snode1    Ready,SchedulingDisabled   <none>          134m   v1.28.15
k8snode2    Ready                      <none>          134m   v1.28.15
root@k8smaster:~#
```

使用命令 `kubectl uncordon <nodeName>` 取消设置

```shell
root@k8smaster:~# kubectl uncordon k8snode1
node/k8snode1 uncordoned
root@k8smaster:~# kubectl get no
NAME        STATUS   ROLES           AGE    VERSION
k8smaster   Ready    control-plane   136m   v1.28.15
k8snode1    Ready    <none>          135m   v1.28.15
k8snode2    Ready    <none>          135m   v1.28.15
root@k8smaster:~# 
```

### drain 排出

> `drain` 是一个更进一步的命令，用于**清空**一个节点上的所有 Pod。
> 首先会像 `kubectl cordon` 一样，将节点标记为**不可调度（Unschedulable）**，阻止新的 Pod 调度上来，然后尝试驱逐（Evict）该节点上所有由 StatefulSet、ReplicaSet、Deployment 或 Job 等控制器管理的 Pod。

它的主要用途是**在对节点进行维护之前，安全地将节点上的所有工作负载迁移走**，确保节点完全空闲，可以进行关机、重启、升级等操作，而不会中断应用服务（前提是你的应用有控制器管理，并且有足够的副本来保证高可用）。

我们使用 `deploy` 创建一个 1 副本的服务

deploy-test.yaml

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-on-node1
spec:
  replicas: 1 # 我们只需要一个副本
  selector:
    matchLabels:
      app: nginx-demo
  template:
    metadata:
      labels:
        app: nginx-demo
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80
```

运行后查看 Pod 列表，确定 Pod 运行的 node

```shell
root@k8smaster:~# kubectl apply -f deploy-test.yaml 
deployment.apps/nginx-on-node1 created
root@k8smaster:~# kubectl get po -o wide
NAME                              READY   STATUS    RESTARTS   AGE     IP               NODE       NOMINATED NODE   READINESS GATES
nginx-on-node1-5455554969-98rm5   1/1     Running   0          8s      10.244.249.9     k8snode1   <none>           <none>
test-pod                          1/1     Running   0          3h10m   10.244.185.195   k8snode2   <none>           <none>
root@k8smaster:~#
```

对 node1 执行 `drain <nodeName>` 命令

```shell
root@k8smaster:~# kubectl drain k8snode1 --ignore-daemonsets 
node/k8snode1 already cordoned
Warning: ignoring DaemonSet-managed Pods: calico-system/calico-node-kr4gh, calico-system/csi-node-driver-7kw4r, kube-system/kube-proxy-lvwxf
evicting pod default/nginx-on-node1-5455554969-98rm5
evicting pod calico-system/calico-typha-677995dc9-l54dz
pod/nginx-on-node1-5455554969-98rm5 evicted
pod/calico-typha-677995dc9-l54dz evicted
node/k8snode1 drained

root@k8smaster:~# kubectl get no
NAME        STATUS                     ROLES           AGE     VERSION
k8smaster   Ready                      control-plane   3h21m   v1.28.15
k8snode1    Ready,SchedulingDisabled   <none>          3h21m   v1.28.15
k8snode2    Ready                      <none>          3h21m   v1.28.15

root@k8smaster:~# kubectl get po -o wide
NAME                              READY   STATUS    RESTARTS   AGE     IP               NODE       NOMINATED NODE   READINESS GATES
nginx-on-node1-5455554969-lpg5h   1/1     Running   0          48s     10.244.185.198   k8snode2   <none>           <none>
test-pod                          1/1     Running   0          3h12m   10.244.185.195   k8snode2   <none>           <none>
root@k8smaster:~# 
```

可以看到 `k8snode1` 被标记为不可调度 `condoned`，且 pod 被杀死并转移到其他节点

取消 `k8snode1` 的不可调度

```shell
root@k8smaster:~# kubectl uncordon k8snode1
node/k8snode1 uncordoned
root@k8smaster:~# kubectl get no
NAME        STATUS   ROLES           AGE     VERSION
k8smaster   Ready    control-plane   3h23m   v1.28.15
k8snode1    Ready    <none>          3h22m   v1.28.15
k8snode2    Ready    <none>          3h22m   v1.28.15
root@k8smaster:~# 
```


### taints 污点

> **Taints (污点)** 是设置在**节点 (Node)** 上的一个属性。
> 
> 它们用来确保节点不会被不适合在该节点上运行的 Pods 使用。比如，你可以给一个有特殊硬件（如 GPU）的节点打上污点，这样只有需要使用 GPU 的 Pods 才能调度到这个节点上。

Taint 有三种效果`<effect>`：
- **NoSchedule:**  **新的** Pods 如果没有容忍这个 Taint，将**不会**被调度到这个节点上。已经运行在该节点上的 Pod 不受影响。
- **PreferNoSchedule:** 调度器会**尽量避免**将没有容忍这个 Taint 的 Pods 调度到这个节点上，但如果实在没有其他节点可用，也可能会调度上来。
- **NoExecute:** 如果一个 Pod 没有容忍这个 Taint，它将**不会**被调度到这个节点上。更重要的是，如果一个 Pod **已经**运行在这个节点上，而节点被打上了 `NoExecute` 污点，那么该 Pod 会被**驱逐**（踢出）这个节点

k8smaster 默认标识为污点，进行查看：

```shell
root@k8smaster:~# kubectl describe nodes k8smaster | grep Taint
Taints:             node-role.kubernetes.io/control-plane:NoSchedule
root@k8smaster:~# 
```


使用 `kubectl taint nodes <key>=<value>:<effect>` 命令给节点添加污点

```shell
kubectl taint nodes k8snode1 special=hardware:NoSchedule

root@k8smaster:~# kubectl taint nodes k8snode1 special=hardware:NoSchedule
node/k8snode1 tainted
root@k8smaster:~# kubectl describe nodes k8snode1 | grep Taint
Taints:             spec=unstable:NoSchedule
```

我们尝试调度一个没有容忍的 Pod 到该节点

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nodeselector-pod
spec:
  nodeSelector:
    kubernetes.io/hostname: k8snode1 # 要求节点必须有 label: kubernetes.io/hostname=k8snode1

  containers:
  - name: nginx
    image: nginx:latest
    ports:
    - containerPort: 80
```

```shell
root@k8smaster:~# kubectl apply -f test-pod.yaml 
pod/plain-pod created

root@k8smaster:~# kubectl get po -o wide
NAME               READY   STATUS    RESTARTS   AGE   IP       NODE     NOMINATED NODE   READINESS GATES
nodeselector-pod   0/1     Pending   0          6s    <none>   <none>   <none>           <none>

root@k8smaster:~# kubectl describe pod nodeselector-pod | grep -A 5 Events
Events:
  Type     Reason            Age                  From               Message
  ----     ------            ----                 ----               -------
  Warning  FailedScheduling  4m40s (x9 over 45m)  default-scheduler  0/3 nodes are available: 1 node(s) didn't match Pod's node affinity/selector, 1 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 1 node(s) had untolerated taint {spec: unstable}. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling..
```

查看日志，发现资源将无法创建，如果我们创建一个带容忍的 Pod

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: tolerated-pod
spec:
  nodeSelector:
    kubernetes.io/hostname: k8snode1
  containers:
  - name: nginx
    image: nginx:latest
    ports:
    - containerPort: 80
  tolerations: # <-- 这里是容忍部分
  - key: "spec" # 容忍的污点键
    operator: "Equal" # 匹配方式，Equal表示键和值都必须匹配
    value: "unstable" # 容忍的污点值
    effect: "NoSchedule" # 容忍的污点效果
```

```shell
root@k8smaster:~# kubectl describe nodes k8snode1 | grep Taint
Taints:             spec=unstable:NoSchedule
root@k8smaster:~# kubectl apply -f test-pod-toler.yaml 
pod/tolerated-pod created
root@k8smaster:~# kubectl get po -o wide
NAME               READY   STATUS    RESTARTS   AGE   IP              NODE       NOMINATED NODE   READINESS GATES
nodeselector-pod   0/1     Pending   0          45m   <none>          <none>     <none>           <none>
tolerated-pod      1/1     Running   0          9s    10.244.249.15   k8snode1   <none>           <none>
```

有容忍 `tolerations` 是可以正常跑在 k8snode1 中的。

移除刚刚添加的 `Taints` 标签

```shell
root@k8smaster:~# kubectl describe nodes k8snode1 | nl | grep Taints
    15  Taints:             spec=unstable:NoSchedule
root@k8smaster:~# 
root@k8smaster:~# kubectl taint node k8snode1 spec-
node/k8snode1 untainted
root@k8smaster:~# 
root@k8smaster:~# kubectl describe nodes k8snode1 | nl | grep Taints
    15  Taints:             <none>
```

### 总结

Cordon 警戒
- 将节点标记为不可调度
- 阻止新的 Pod 调度到该节点，已存在的 Pod 不受影响

Drain 排出
- 将节点标记为不可调度
- 阻止新的 Pod 调度到该节点，并终止已运行的 Pods（由控制器在其它节点重新创建）

Taints 污点
- 设置节点的属性，排斥没有相应**容忍**的 Pod
- 没有相应容忍的 Pod 会创建在其他节点
- 可以用于专用工作负载，例如 GPU 节点