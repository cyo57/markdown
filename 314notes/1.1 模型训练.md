


## 前期准备

安装必要的库
```python
import tensorflow as tf
from tensorflow.keras.preprocessing import image_dataset_from_directory
from pathlib import Path
import matplotlib.pyplot as plt
import numpy as np
```

数据集准备
- 训练集 - A、B、C各1500张
- 验证集 - A、B、C各500张
- 测试集 - A、B、C各500张
从 [Kaggle](https://kaggle.com) 下载合适的训练数据集

## 数据集训练

设置Matplotlib的参数以支持中文显示和正确显示负号
```python
plt.rcParams['font.sans-serif'] = ['SimHei']  # 设置中文显示
plt.rcParams['axes.unicode_minus'] = False    # 设置负号显示
```

设置数据集路径：
```python
PATH = Path('data')
train_dir = PATH / 'train'
test_dir = PATH / 'test'
val_dir = PATH / 'val'
```

加载数据集：
```python
train_dataset = image_dataset_from_directory(
	train_dir,
	label_mode='categorical',  # 标签模式为分类
	shuffle=True,              # 打乱数据
	batch_size=32,             # 每批次32张图片
	image_size=(256, 256)      # 调整图片大小为256x256
)

validation_dataset = image_dataset_from_directory(
	test_dir,
	label_mode='categorical',  # 标签模式为分类
	shuffle=True,              # 打乱数据
	batch_size=32,             # 每批次32张图片
	image_size=(256, 256)      # 调整图片大小为256x256
)

# 计算验证数据集的批次数量
val_batch = tf.data.experimental.cardinality(validation_dataset)
# 将一部分验证数据集作为测试数据集（六分之一）
test_dataset = validation_dataset.take(val_batch // 6)
# 剩余部分作为验证数据集（六分之五）
validation_dataset = validation_dataset.skip(val_batch // 6)
# 图片分类名
class_name = train_dataset.class_names

# 数据集预读，预防IO瓶颈
AUTOTUNE = 30
train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)
validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)
test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)
```

加强数据集泛用性
```python

```